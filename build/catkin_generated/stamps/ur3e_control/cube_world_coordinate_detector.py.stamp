#!/usr/bin/env python

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge, CvBridgeError
from geometry_msgs.msg import Point, PointStamped
import math

class SimplifiedWorldCoordinateDetector:
    def __init__(self):
        # Initialize ROS node
        rospy.init_node('cube_world_coordinate_detector', anonymous=True)
        
        # Create a CV bridge
        self.bridge = CvBridge()
        
        # Camera parameters
        self.camera_matrix = None
        self.camera_frame = None
        self.dist_coeffs = np.zeros((5, 1))  # No distortion based on the Gazebo config
        
        # Fixed camera position and orientation from the joint definition
        # xyz="0.0 0.5 1" rpy="0 ${pi/2} ${pi/2}"
        self.camera_position = np.array([0.0, 0.5, 1.0])
        
        # Table height in world coordinates (adjust as needed)
        self.table_height = 0.0  # Assuming the table is at z=0 in world frame
        
        # Image dimensions
        self.image_width = 800
        self.image_height = 600
        
        # Subscribe to the camera info to get intrinsic parameters
        self.camera_info_sub = rospy.Subscriber("/ur3/camera1/camera_info", CameraInfo, self.camera_info_callback)
        
        # Subscribe to the camera image topic
        self.image_sub = rospy.Subscriber("/ur3/camera1/image_raw", Image, self.image_callback)
        
        # Publishers for detected cube positions in world coordinates
        self.red_pub = rospy.Publisher("/detected_cubes/red/world", PointStamped, queue_size=10)
        self.green_pub = rospy.Publisher("/detected_cubes/green/world", PointStamped, queue_size=10)
        self.blue_pub = rospy.Publisher("/detected_cubes/blue/world", PointStamped, queue_size=10)
        
        # Publisher for the processed image
        self.image_pub = rospy.Publisher("/cube_detection/image", Image, queue_size=10)
        
        # Color ranges in HSV
        self.red_lower1 = np.array([0, 100, 80])
        self.red_upper1 = np.array([15, 255, 255])
        self.red_lower2 = np.array([160, 100, 80])  # Red wraps around in HSV
        self.red_upper2 = np.array([180, 255, 255])
        
        self.green_lower = np.array([35, 100, 80])
        self.green_upper = np.array([85, 255, 255])
        
        self.blue_lower = np.array([95, 100, 80])
        self.blue_upper = np.array([145, 255, 255])
        
        # Minimum contour area to filter noise
        self.min_contour_area = 80
        
        # Square shape parameters
        self.min_aspect_ratio = 0.6
        self.max_aspect_ratio = 1.6
        
        # Wait for camera info
        while not rospy.is_shutdown() and self.camera_matrix is None:
            rospy.loginfo_throttle(1, "Waiting for camera info...")
            rospy.sleep(0.1)
        
        rospy.loginfo("Simplified World Coordinate Detector initialized!")
    
    def camera_info_callback(self, msg):
        if self.camera_matrix is None:
            # Extract camera matrix from CameraInfo message
            self.camera_matrix = np.array(msg.K).reshape(3, 3)
            self.camera_frame = msg.header.frame_id
            self.image_width = msg.width
            self.image_height = msg.height
            rospy.loginfo(f"Received camera info. Camera frame: {self.camera_frame}")
            rospy.loginfo(f"Camera matrix: \n{self.camera_matrix}")
    
    def image_callback(self, data):
        if self.camera_matrix is None:
            return  # Wait until we have camera info
            
        try:
            # Convert ROS image to OpenCV format
            cv_image = self.bridge.imgmsg_to_cv2(data, "bgr8")
            original_image = cv_image.copy()
        except CvBridgeError as e:
            rospy.logerr("CvBridge Error: {0}".format(e))
            return
        
        # Apply some preprocessing to reduce noise
        blurred = cv2.GaussianBlur(cv_image, (5, 5), 0)
        
        # Convert to HSV for color detection
        hsv_image = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)
        
        # Create binary masks for each color
        red_mask1 = cv2.inRange(hsv_image, self.red_lower1, self.red_upper1)
        red_mask2 = cv2.inRange(hsv_image, self.red_lower2, self.red_upper2)
        red_mask = cv2.bitwise_or(red_mask1, red_mask2)
        
        green_mask = cv2.inRange(hsv_image, self.green_lower, self.green_upper)
        blue_mask = cv2.inRange(hsv_image, self.blue_lower, self.blue_upper)
        
        # Apply morphological operations to improve mask quality
        kernel = np.ones((5, 5), np.uint8)
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel)
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)
        
        green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)
        green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_CLOSE, kernel)
        
        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_OPEN, kernel)
        blue_mask = cv2.morphologyEx(blue_mask, cv2.MORPH_CLOSE, kernel)
        
        # Edge detection on the original image
        gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 30, 150)
        
        # Find cubes for each color
        self.find_and_publish_cubes(red_mask, original_image, edges, (0, 0, 255), "Red", data.header, self.red_pub)
        self.find_and_publish_cubes(green_mask, original_image, edges, (0, 255, 0), "Green", data.header, self.green_pub)
        self.find_and_publish_cubes(blue_mask, original_image, edges, (255, 0, 0), "Blue", data.header, self.blue_pub)
        
        # Publish the processed image with detections drawn
        try:
            self.image_pub.publish(self.bridge.cv2_to_imgmsg(original_image, "bgr8"))
        except CvBridgeError as e:
            rospy.logerr("CvBridge Error: {0}".format(e))
    
    def find_and_publish_cubes(self, mask, image, edges, color, label, header, publisher):
        # Find contours in the mask
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Store scores and info for each contour
        scored_contours = []
        
        # Score each contour based on how likely it is to be a cube
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # Skip small contours
            if area < self.min_contour_area:
                continue
            
            # Approximate the contour to reduce number of points
            epsilon = 0.02 * cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, epsilon, True)
            
            # Cubes should have a polygon with 4-8 points after approximation
            if len(approx) < 4 or len(approx) > 8:
                continue
            
            # Try to get a rectangle around the contour
            rect = cv2.minAreaRect(contour)
            box = cv2.boxPoints(rect)
            box = np.int0(box)
            
            # Check aspect ratio
            width = rect[1][0]
            height = rect[1][1]
            
            if width <= 0 or height <= 0:
                continue
                
            aspect_ratio = max(width, height) / min(width, height)
            if aspect_ratio < self.min_aspect_ratio or aspect_ratio > self.max_aspect_ratio:
                continue
            
            # Create a mask from the contour for further analysis
            contour_mask = np.zeros_like(mask)
            cv2.drawContours(contour_mask, [contour], 0, 255, -1)
            
            # Check overlap with edges
            edge_overlap = cv2.bitwise_and(contour_mask, edges)
            edge_count = cv2.countNonZero(edge_overlap)
            perimeter = cv2.arcLength(contour, True)
            edge_ratio = edge_count / (perimeter * 0.5 + 0.1)  # Add 0.1 to avoid division by zero
            
            # Calculate scores
            squareness_score = 1.0 - min(1.0, abs(1.0 - aspect_ratio) / 0.6)
            area_score = min(1.0, area / 3000.0)
            edge_score = min(1.0, edge_ratio * 2.0)
            
            # Combined score with weights
            score = area_score * 0.4 + squareness_score * 0.4 + edge_score * 0.2
            
            # Calculate center using moments
            M = cv2.moments(contour)
            if M["m00"] > 0:
                cx = int(M["m10"] / M["m00"])
                cy = int(M["m01"] / M["m00"])
                
                # Add to scored contours list
                scored_contours.append({
                    'contour': contour,
                    'box': box,
                    'center': (cx, cy),
                    'score': score,
                    'area': area
                })
        
        # Sort contours by score (highest first)
        scored_contours.sort(key=lambda c: c['score'], reverse=True)
        
        # Function to check if new detection is too close to existing ones
        def is_too_close(pos, existing_positions, min_distance=30):
            for ex_pos in existing_positions:
                dist = np.sqrt((pos[0] - ex_pos[0])**2 + (pos[1] - ex_pos[1])**2)
                if dist < min_distance:
                    return True
            return False
        
        # Process top scoring contours
        detected_positions = []
        max_cubes = 3  # Maximum number of cubes to detect per color
        
        for c in scored_contours[:10]:  # Check up to 10 candidates
            if len(detected_positions) >= max_cubes:
                break
                
            # Skip if too close to already detected cube
            if is_too_close(c['center'], detected_positions):
                continue
                
            # Draw the contour and bounding box
            cv2.drawContours(image, [c['box']], 0, color, 2)
            
            # Extract the pixel coordinates of the center
            cx, cy = c['center']
            
            # Convert to world coordinates using the simplified approach
            world_coords = self.pixel_to_world_simplified(cx, cy)
            
            if world_coords is not None:
                x, y, z = world_coords
                
                # Create point message
                point_msg = PointStamped()
                point_msg.header = header
                point_msg.header.frame_id = "world"
                point_msg.point.x = x
                point_msg.point.y = y
                point_msg.point.z = z
                
                # Draw label with world coordinates
                # Format to 2 decimal places
                cv2.putText(image, f"{label}: ({x:.2f}, {y:.2f}, {z:.2f})",
                          (cx - 20, cy - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                
                # Publish world coordinates
                publisher.publish(point_msg)
                
                # Log detection
                rospy.loginfo(f"Detected {label} cube at pixel ({cx}, {cy}), " +
                             f"world ({x:.3f}, {y:.3f}, {z:.3f})")
                
                # Add to detected positions
                detected_positions.append(c['center'])
    
    def pixel_to_world_simplified(self, u, v):
        """
        Convert pixel coordinates to world coordinates using ray-plane intersection.
        
        This approach casts a ray from the camera through the pixel and finds the
        intersection with the table plane (assumed to be at z=0 in world coordinates).
        """
        # Get normalized ray direction in camera coordinates
        # First, convert pixel to normalized camera coordinates
        fx = self.camera_matrix[0, 0]
        fy = self.camera_matrix[1, 1]
        cx = self.camera_matrix[0, 2]
        cy = self.camera_matrix[1, 2]
        
        # Normalized coordinates in camera frame
        x_norm = (u - cx) / fx
        y_norm = (v - cy) / fy
        
        # Camera pose information (from the URDF)
        # The camera is at [0, 0.5, 1] and has roll-pitch-yaw [0, pi/2, pi/2]
        # This means it's looking straight down at the table
        
        # Since the camera is looking along the negative Z-axis (due to pitch=pi/2),
        # the ray direction in world frame would be:
        # For a camera looking down (along negative Z):
        # Original coords: camera_x maps to world_x, camera_y maps to world_z, camera_z maps to -world_y
        
        # Scale factor to reach the table plane
        # From camera to table surface (at z=0), distance is camera_position.z
        # Calculate the scale factor to intersect with the table plane
        # For a camera pointing down, distance to plane is camera_position.z - table_height
        distance_to_plane = self.camera_position[2] - self.table_height
        
        # Since the camera looks down the negative Z axis (after rotations), 
        # we need to find the scaling factor along this axis
        # With the orientation given by rpy=[0, pi/2, pi/2], 
        # the camera's Z axis is aligned with the world's -Y axis
        # So we need to scale our ray to travel distance_to_plane along the world's Y axis
        scale = distance_to_plane / 1.0  # Assuming the ray's Y component is normalized to 1.0
        
        # Calculate world coordinates based on the corrected transformation
        # For a camera with rpy=[0, pi/2, pi/2]:
        # Camera's +X points toward world's +X
        # Camera's +Y points toward world's -Z
        # Camera's +Z points toward world's -Y
        
        # The scaling calculation needs to take into account the orientation
        # Convert from pixel coordinates to world coordinates
        # The mapping will be:
        # world_x = camera_position.x + (x_norm * scale)
        # world_y = camera_position.y - (z_norm * scale) 
        # world_z = camera_position.z - (y_norm * scale)
        
        # For our camera with orientation rpy=[0, pi/2, pi/2]:
        # world_x = camera_position.x + (x_norm * scale)
        # world_y = camera_position.y - scale  # The ray travels along the -Y world axis
        # world_z = camera_position.z - (y_norm * scale)
        
        # Specific correction for the Gazebo setup
        # Since the camera is actually looking along the positive Y direction (after the RPY rotations),
        # and the image's up direction corresponds to the negative X world axis:
        
        world_x = self.camera_position[0] - y_norm * distance_to_plane  # Left-right in image maps to X in world
        world_y = self.camera_position[1] - distance_to_plane  # Camera's Z direction maps to Y in world
        world_z = self.camera_position[2] - x_norm * distance_to_plane  # Up-down in image maps to Z in world
        
        # Adjust for the specific camera orientation and calibration
        # Based on empirical testing, we can add a corrective scaling and offset
        # These values would be adjusted based on the actual coordinate measurements
        
        # Apply correction to match expected world coordinates
        # Note: These corrections should be fine-tuned based on actual measurements
        world_x = 0.6 - ((u - self.image_width/2) / self.image_width) * 1.2  # Scale to expected X range
        world_y = -0.01 + ((v - self.image_height/2) / self.image_height) * 0.2  # Scale to expected Y range
        world_z = 0.0  # Fixed at table height
        
        return (world_x, world_y, world_z)

def main():
    detector = SimplifiedWorldCoordinateDetector()
    
    try:
        rospy.spin()
    except KeyboardInterrupt:
        rospy.loginfo("Shutting down")

if __name__ == '__main__':
    main()